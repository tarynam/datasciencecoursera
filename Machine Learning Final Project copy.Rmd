---
title: "Machine Learning Final Project"
author: "Taryn McLaughlin"
date: "10/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = TRUE, error = TRUE)
```


```{r load data, warning = FALSE}
setwd("/Users/tarynam/Desktop/Machine Learning Final Project")
train<-read.csv("pml-training.csv")
test<-read.csv("pml-testing.csv")
```

##Pre-processing the Training Data Set

The training data set is quite large with `r dim(train)[1]` observations of `r dim(train)[2]` variables. If we were to include all variables in our prediction models, the computation time would be extremely long, so we will want to inspect our data set and try to prune it if possible.

###Missing Data
```{r missing values}
library(dplyr)
library(knitr)
missing_data<-data.frame(sapply(train, function(x){sum(is.na(x))}))
missing_data$variable<-row.names(missing_data)
missing_data<-dplyr::rename(missing_data, missing = sapply.train..function.x...)%>%
    filter(missing>0)%>%
    select(variable, missing)
kable(head(missing_data))
```

There are `r length(missing_data$variable)` variables with `r missing_data[1,2]` missing values which can interfere with models such as random forest. As such, we will trim the training data set to only include variables which have a complete set of observations.

```{r}
train_noNA<-train[ , colSums(is.na(train)) == 0] #sum up the number of NAs in each column
#create new data table that includes only those columns where the sum is equal to zero
```

###Zero and Near Zero Variance
When building models, we need to be concerned about zero variance and near zero variance predictors. These can skew models and make them unstable, particularly when cross-validation is used. Here we generate the Frequency Ratio and the Percent Unique Values. The first is the ratio of the frequency of the most common value to the second most common value for a feature- this should be close to 1 for well balanced predictors. The second is the number of unique values divided by the total number of entries x100- this will be close to 100 when there are numerous unique predictors and close to 0 when there are only a few. 

```{r zero and near zero variance}
library(caret)
library(dplyr)
library(knitr)
#Identify the relevant measurements to detect  for each variable
nz<-nearZeroVar(train_noNA, saveMetrics= TRUE)
nz<-dplyr::arrange(nz, desc(freqRatio))
kable(head(nz))
```

Features that have a high value in the Frequency Ratio (i.e. one value dominates that feature) and a low value in the Percent Unique Values (i.e. there are a lot of different values) have near-zero variance. As we can see, there are indeed features that have non-zero variance. We may want to account for this by including a pre-processing step when building our models.

###Correlated Variables
To determine whether any variables are highly correlated with one another- and by extension whether we should reduce the dimensions of our data with pre-processing, I have calculated the correlation matrix for all numeric variables in the data set.

```{r correlation}
numeric.only <- function(X,...){
    returnCols <- rep(FALSE, ncol(X))
    a<-sapply(X, class)
    returnCols[a == "numeric"] <- TRUE
    return(X[a =="numeric"])}
numbers<-numeric.only(train_noNA)
descrCor <-  cor(numbers)
heatmap(descrCor)
```

There seem to be a few variables correlated with one another as indicated by the red boxes in the grid. To be safe, since we have an abundance of predictors to work with, I will pre-process this data into its principal components. Since the random forest method in caret returns an error when principle components are utilized inline, we will create a second training data set- train_preProc -that has already been pre-processed. This will exclude variables with near zero variance and uses principle components to transform the data to a smaller subâ€“space where the new variable are uncorrelated with one another. This may be useful computationally due to the large size of the data set.

```{r pre processing data, eval = FALSE}
library(caret)
train_process<-preProcess(train_noNA, method = c("nzv", "pca"))
train_preProc<-predict(train_process, train_noNA)
```

Due to model performance, I will preemptively subset my data such that x is a table of predictors and y is the outcome variable to use in model training later.

```{r}
n<-length(train_preProc)
x<-train_preProc[,-n]
y<-train_preProc[,n]
```


##Training Models

###Training methods
We now specify our training method for all model specifications. To lower the computation time on these models, we are doing 5-fold cross validation with no repeats. We will also allow for parallel processing.

```{r, eval = FALSE}
library(caret)
fitControl <- trainControl(## 5-fold CV
    method = "cv", #cv: cross validation
    number = 5, #the number of folds
    allowParallel = TRUE
    )
```


###Neural Networks

```{r nnet,  cache=TRUE}
library(caret)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
set.seed(825)
fitControl <- trainControl(## 5-fold CV
    method = "cv", #cv: cross validation
    number = 5, #the number of folds
    allowParallel = TRUE
    )
fit_nnet <- train(x,y,
                  method="nnet",
                  trControl = fitControl, 
                  verbose=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

###Random Forest

```{r random forest,  cache=TRUE}
library(caret)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
set.seed(825)
fitControl <- trainControl(## 5-fold CV
    method = "cv", #cv: cross validation
    number = 5, #the number of folds
    allowParallel = TRUE
    )
fit_rf <- train(x,y, 
                 method = "rf", 
                 trControl = fitControl,
                 verbose = FALSE)
stopCluster(cluster)
registerDoSEQ()
```

###Gradient Boosting Machine

```{r gradient boosting machine,  cache=TRUE}
library(caret)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
set.seed(825)
fitControl <- trainControl(## 5-fold CV
    method = "cv", #cv: cross validation
    number = 5, #the number of folds
    allowParallel = TRUE
    )
fit_gbm <- train(x,y,
                  method="gbm",
                  trControl = fitControl, 
                  verbose=FALSE)
stopCluster(cluster)
registerDoSEQ()
```

###Support Vector Matrices

```{r support vector machines,  cache=TRUE}
library(caret)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
set.seed(825)
fitControl <- trainControl(## 5-fold CV
    method = "cv", #cv: cross validation
    number = 5, #the number of folds
    allowParallel = TRUE
    )
fit_svm <- train(x,y,
                  method="svmLinear",
                  trControl = fitControl, 
                  verbose=FALSE)
stopCluster(cluster)
registerDoSEQ()
```


###K-nearest Neighbors

```{r K nearest neighbors,  cache=TRUE}
library(caret)
library(doParallel)
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)
set.seed(825)
fitControl <- trainControl(## 5-fold CV
    method = "cv", #cv: cross validation
    number = 5, #the number of folds
    allowParallel = TRUE
    )
fit_knn <- train(x,y,
                  method="knn",
                  trControl = fitControl, 
                  verbose=FALSE)
stopCluster(cluster)
registerDoSEQ()
```



```{r}
library(knitr)
library(caret)
fit_nnet
fit_nnet$resample
confusionMatrix.train(fit_nnet)

fit_rf
fit_rf$resample
confusionMatrix.train(fit_rf)

fit_gbm
fit_gbm$resample
confusionMatrix.train(fit_gbm)

fit_svm
fit_svm$resample
confusionMatrix.train(fit_svm)

fit_knn
fit_knn$resample
confusionMatrix.train(fit_knn)
```

